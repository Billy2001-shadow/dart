import argparse
import logging
import os
import pprint
import random

import torch
import torch.backends.cudnn as cudnn
from torch.utils.data import DataLoader
from torch.optim import AdamW
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter

from dataset.nyud import NYUD  # è®­ç»ƒé›†
from dataset.kitti import get_kitti_loader

from network.dpt import TinyVimDepth
from util.loss import SiLogLoss
from util.metric import eval_depth
from util.config_loader import load_config
from util.utils import init_log, count_parameters

parser = argparse.ArgumentParser(description='TinyVim Depth for Metric Depth Estimation')
parser.add_argument('--config', default='configs/nyud/train.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
args = parser.parse_args()
args = load_config(args)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


def main():
    os.makedirs(args.save_path, exist_ok=True)
    os.makedirs(args.log_directory, exist_ok=True)

    logger = init_log('global', logging.INFO)
    logger.propagate = 0
    all_args = {**vars(args)}
    logger.info('{}\n'.format(pprint.pformat(all_args)))
    writer = SummaryWriter(args.log_directory)
    cudnn.enabled = True
    cudnn.benchmark = True

    #################################################################### DataLoader ####################################################################
    
    if args.dataset == 'NYUD':
        size = (args.img_w, args.img_h)
        trainset = NYUD(args.trainset_path, 'train', size=size,augment_camera_intrinsics=args.augment_camera_intrinsics)
        trainloader = DataLoader(trainset, batch_size=args.batch_size, pin_memory=True, num_workers=args.workers,
                                 drop_last=True)
        valset = NYUD(args.valset_path, 'val', size=size)
        valloader = DataLoader(valset, batch_size=1, pin_memory=True, num_workers=1, drop_last=True)
    elif args.dataset == 'KITTI':
        size = (args.img_size, args.img_size)
        trainloader = get_kitti_loader(args.trainset_path, 'train', size=size) # torch.Size([16, 3, 448, 1472])
        valloader = get_kitti_loader(args.valset_path,'val',size=size)
        
    else:
        raise NotImplementedError
    #################################################################### DataLoader ####################################################################

    ###################################################################  Model Load ####################################################################
    model = TinyVimDepth(max_depth=args.max_depth)  # å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU
                         
                         
    if args.pretrained_from:
        checkpoint = torch.load(args.pretrained_from, map_location='cpu')
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint

        #missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        missing_keys, unexpected_keys = model.load_state_dict({k: v for k, v in state_dict.items() if 'pretrained' in k}, strict=False)
        print(f"Loaded pretrained weights from {args.pretrained_from}")
        if missing_keys:
            print(f"Missing keys: {missing_keys}")
        if unexpected_keys:
            print(f"Unexpected keys: {unexpected_keys}")
        # æ£€æŸ¥æ‰€æœ‰å‚æ•°æ˜¯å¦è¢«åŠ è½½
        total_params = sum(p.numel() for p in model.parameters())
        loaded_params = sum(p.numel() for k, p in model.named_parameters() if k in state_dict)
        print(f"\nå‚æ•°æ€»æ•°: {total_params}, å·²åŠ è½½å‚æ•°: {loaded_params} (å æ¯”: {loaded_params / total_params:.1%})")

        # åœ¨åŠ è½½æƒé‡åæ·»åŠ ä»¥ä¸‹ä»£ç 
        print("\n=== Pretrained Weight Verification ===")

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å†»ç»“ backbone
    if getattr(args, "freeze_backbone", False):
        frozen = 0
        for name, param in model.named_parameters():
            if 'pretrained' in name:
                param.requires_grad = False
                frozen += param.numel()
        logger.info(f"Backbone frozen, params frozen: {frozen:,}")

    model.to(device)
    pretrained_params, head_params, total_params = count_parameters(model)
    logger.info(f"Model Parameters Summary:")
    logger.info(f"Pretrained (TinyViM) parameters: {pretrained_params:,}")
    logger.info(f"Depth Head parameters: {head_params:,}")
    logger.info(f"Total parameters: {total_params:,}")
    ####################################################################################################################################################

    ###################################################################  Loss &&  Optimizer  ###########################################################
    criterion = SiLogLoss().to(device)

    backbone_params = [param for name, param in model.named_parameters() if 'pretrained' in name and param.requires_grad]
    head_params = [param for name, param in model.named_parameters() if 'pretrained' not in name and param.requires_grad]

    param_groups = []
    if backbone_params:
        param_groups.append({'params': backbone_params, 'lr': args.lr, 'name': 'backbone'})
    if head_params:
        param_groups.append({'params': head_params, 'lr': args.lr * 10.0, 'name': 'head'})

    optimizer = AdamW(param_groups, lr=args.lr, betas=(0.9, 0.999), weight_decay=0.01)

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f'å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({trainable_params / total_params * 100:.1f}%)')
    #####################################################################################################################################################
    total_iters = args.epochs * len(trainloader)

    previous_best = {'d1': 0, 'd2': 0, 'd3': 0, 'abs_rel': 100, 'sq_rel': 100, 'rmse': 100, 'rmse_log': 100,
                     'log10': 100, 'silog': 100}
    
    # æ·»åŠ æœ€ä½³d1å€¼è·Ÿè¸ª
    best_d1 = 0.0
    best_d1_epoch = 0

    for epoch in range(args.epochs):
        logger.info(
            '===========> Epoch: {:}/{:}, d1: {:.3f}, d2: {:.3f}, d3: {:.3f}'.format(epoch, args.epochs,
                                                                                     previous_best['d1'],
                                                                                     previous_best['d2'],
                                                                                     previous_best['d3']))
        logger.info('===========> Epoch: {:}/{:}, abs_rel: {:.3f}, sq_rel: {:.3f}, rmse: {:.3f}, rmse_log: {:.3f}, '
                    'log10: {:.3f}, silog: {:.3f}'.format(
            epoch, args.epochs, previous_best['abs_rel'], previous_best['sq_rel'], previous_best['rmse'],
            previous_best['rmse_log'], previous_best['log10'], previous_best['silog']))

        model.train()
        total_loss = 0

        for i, sample in enumerate(trainloader):
            optimizer.zero_grad()

            img, depth, valid_mask = sample['image'].cuda(), sample['depth'].cuda(), sample['valid_mask'].cuda()

            if random.random() < 0.5:
                img = img.flip(-1)
                depth = depth.flip(-1)
                valid_mask = valid_mask.flip(-1)

            pred = model(img)

            loss = criterion(pred, depth, (valid_mask == 1) & (depth >= args.min_depth) & (depth <= args.max_depth))

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            iters = epoch * len(trainloader) + i

            lr = args.lr * (1 - iters / total_iters) ** 0.9

            for g in optimizer.param_groups:
                if g.get('name') == 'backbone':
                    g["lr"] = lr
                else:
                    g["lr"] = lr * 10.0

            writer.add_scalar('train/loss', loss.item(), iters)

            if i % 100 == 0:
                logger.info('Iter: {}/{}, LR: {:.7f}, Loss: {:.3f}'.format(i, len(trainloader),
                                                                           optimizer.param_groups[0]['lr'], loss.item()))

        model.eval()

        results = {'d1': 0.0, 'd2': 0.0, 'd3': 0.0,
                   'abs_rel': 0.0, 'sq_rel': 0.0, 'rmse': 0.0,
                   'rmse_log': 0.0, 'log10': 0.0, 'silog': 0.0}
        nsamples = 0

        for i, sample in enumerate(valloader):
            img, depth, valid_mask = sample['image'].cuda().float(), sample['depth'].cuda()[0], sample['valid_mask'].cuda()[0]

            with torch.no_grad():
                pred = model(img)
                pred = F.interpolate(pred[:, None], depth.shape[-2:], mode='bilinear', align_corners=True)[0, 0]
            
                eigen_crop_mask = torch.zeros_like(depth, dtype=torch.bool, device=depth.device)
                if args.dataset == 'NYUD':
                    eigen_crop_mask[45: 471, 41: 601] = True
                elif args.dataset == 'KITTI':
                    eigen_crop_mask[153:371, 44:1197] = True # # (218, 1153)
                else:
                    raise NotImplementedError
                    
                depth_mask  = (depth >= args.min_depth) & (depth <= args.max_depth)
                valid_mask = eigen_crop_mask & depth_mask

                if valid_mask.sum() < 10:
                    continue

                cur_results = eval_depth(pred[valid_mask], depth[valid_mask])

                for k in results.keys():
                    results[k] += cur_results[k]
                nsamples += 1

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        for k in results.keys():
            results[k] /= nsamples

        logger.info('==========================================================================================')
        logger.info('{:>8}, {:>8}, {:>8}, {:>8}, {:>8}, {:>8}, {:>8}, {:>8}, {:>8}'.format(*tuple(results.keys())))
        logger.info('{:8.3f}, {:8.3f}, {:8.3f}, {:8.3f}, {:8.3f}, {:8.3f}, {:8.3f}, {:8.3f}, {:8.3f}'.format(
            *tuple([results[k] for k in results.keys()])))
        logger.info('==========================================================================================')
        print()

        for name, metric in results.items():
            writer.add_scalar(f'eval/{name}', metric, epoch)

        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        for k in results.keys():
            if k in ['d1', 'd2', 'd3']:
                previous_best[k] = max(previous_best[k], results[k])
            else:
                previous_best[k] = min(previous_best[k], results[k])
                
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³d1
        current_d1 = results['d1']
        if current_d1 > best_d1:
            best_d1 = current_d1
            best_d1_epoch = epoch
            logger.info(f'âœ¨ å‘ç°æ–°çš„æœ€ä½³d1: {current_d1:.4f} (epoch: {epoch})')
            
            # ä¿å­˜æœ€ä½³d1æ¨¡å‹
            best_checkpoint = {
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'epoch': epoch,
                'd1': best_d1,
                'previous_best': previous_best,
                'args': args
            }     
            # åŒæ—¶ä¿å­˜ä¸ºbest_d1.pthï¼ˆè¦†ç›–ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ï¼‰
            torch.save(best_checkpoint, os.path.join(args.save_path, 'best_d1.pth'))
            logger.info(f'ğŸ’¾ å·²æ›´æ–°best_d1.pth (å½“å‰æœ€ä½³: {current_d1:.4f})')

        # ä¿å­˜å½“å‰epochçš„æ¨¡å‹
        checkpoint = {
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'epoch': epoch,
            'current_metrics': results,
            'previous_best': previous_best,
            'args': args
        }
        torch.save(checkpoint, os.path.join(args.save_path, f'latest_epoch{epoch}.pth'))
        logger.info(f'ğŸ“ å·²ä¿å­˜epoch {epoch}çš„æ¨¡å‹')

    # è®­ç»ƒç»“æŸæ—¶è¾“å‡ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
    logger.info(f'ğŸ† è®­ç»ƒå®Œæˆï¼æœ€ä½³d1: {best_d1:.4f} (æ¥è‡ªepoch {best_d1_epoch})')
    logger.info(f'æœ€ä½³æ¨¡å‹å·²ä¿å­˜ä¸º: {os.path.join(args.save_path, "best_d1.pth")}')


if __name__ == '__main__':
    main()
    
